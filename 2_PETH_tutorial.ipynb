{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34403de-d699-4189-8afb-bfb0933e9d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pynapple matplotlib dandi dandischema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387db77a",
   "metadata": {},
   "source": [
    "# Peri-Event Time Histogram Tutorial \n",
    "\n",
    "*Author: Dhruv Mehrotra*\n",
    "\n",
    "\n",
    "This notebook demonstrates how we use Pynapple on various publicly available datasets in systems neuroscience to streamline analysis. In this notebook, we will examine the dataset from <a href=\"https://www.nature.com/articles/s41593-022-01020-w\" target=\"_blank\">Zheng et al (2022)</a>, which was used to generate Figure 4c in our <a href=\"https://elifesciences.org/articles/85786\" target=\"_blank\">publication</a>. \n",
    "\n",
    "The NWB file for the example used here is provided in <a href=\"https://github.com/PeyracheLab/pynacollada/tree/main/pynacollada/Pynapple%20Paper%20Figures/Zheng%202022/000207/sub-4\" target=\"_blank\">this</a> repository. The entire dataset can be downloaded  <a href=\"https://dandiarchive.org/dandiset/000207/0.220216.0323\" target=\"_blank\">here</a>. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207b40ab",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Note:** This tutorial uses matplotlib for displaying the figure as well as the dandi package\n",
    "\n",
    "You can install all with `pip install matplotlib dandi dandischema`\n",
    "\n",
    "***\n",
    "\n",
    "First, import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b93706",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import matplotlib.pyplot, numpy and pynapple libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1bc46e",
   "metadata": {},
   "source": [
    "The next step is to stream the data from the DANDI server. To do so, run the following lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7b5584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import more libraries\n",
    "from pynwb import NWBHDF5IO\n",
    "\n",
    "from dandi.dandiapi import DandiAPIClient\n",
    "import fsspec\n",
    "from fsspec.implementations.cached import CachingFileSystem\n",
    "import h5py\n",
    "\n",
    "# Enter the session ID and path to the file\n",
    "dandiset_id, filepath = (\"000207\", \"sub-4/sub-4_ses-4_ecephys.nwb\")\n",
    "\n",
    "with DandiAPIClient() as client:\n",
    "    asset = client.get_dandiset(dandiset_id, \"draft\").get_asset_by_path(filepath)\n",
    "    s3_url = asset.get_content_url(follow_redirects=1, strip_query=True)\n",
    "\n",
    "# First, create a virtual filesystem based on the http protocol\n",
    "fs = fsspec.filesystem(\"http\")\n",
    "\n",
    "# Create a cache to save downloaded data to disk (optional)\n",
    "fs = CachingFileSystem(\n",
    "    fs=fs,\n",
    "    cache_storage=\"nwb-cache\",  # Local folder for the cache\n",
    ")\n",
    "\n",
    "# Next, open the file\n",
    "file = h5py.File(fs.open(s3_url, \"rb\"))\n",
    "io = NWBHDF5IO(file=file, load_namespaces=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d937cb7d",
   "metadata": {},
   "source": [
    "Now let's load the data from the Neurodata Without Borders (NWB) file. This is done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0039b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\n",
    "sns.set_theme(style=\"ticks\", palette=\"colorblind\", font_scale=1.5, rc=custom_params)\n",
    "\n",
    "data = nap.NWBFile(io.read())  # Load the NWB file for this dataset\n",
    "\n",
    "# What does this look like?\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e53d5d6",
   "metadata": {},
   "source": [
    "This dataset has several variables of interest. We will first get the spike times and visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6d063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get spike timings\n",
    "spikes = \n",
    "\n",
    "# What does this look like?\n",
    "print(spikes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bd7012",
   "metadata": {},
   "source": [
    "The spike times TsGroup has, among other information, the mean firing rate of the unit, the X, Y and Z coordinates, the brain region the unit was recorded from, and the channel number on which the unit was located.\n",
    "\n",
    "Next, let's get the encoding table of all stimulus times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdd55b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_table = \n",
    "\n",
    "# What does this look like?\n",
    "print(encoding_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4202df5c",
   "metadata": {},
   "source": [
    "This table has, among other things, the scene boundary times for which we will plot the peri-event time histogram (PETH).\n",
    "\n",
    "There are 3 types of scene boundaries in this data. For the purposes of demonstration, we will use only the \"No boundary\" (NB) and the \"Hard boundary\" (HB conditions). The encoding table has a stimCategory field, which tells us the type of boundary corresponding to a given trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90434e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the scene boundary type for all trials\n",
    "stimCategory = np.array(encoding_table.stimCategory)\n",
    "\n",
    "# What does this look like?\n",
    "print(stimCategory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfdb3ff",
   "metadata": {},
   "source": [
    "Trials marked 0 correspond to NB, while trials marked 2 correspond to HB. Let's extract the trial numbers for NB and HB trials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6437181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB trial indices\n",
    "indxNB = np.where(stimCategory == 0)  \n",
    "\n",
    "# HB trial indices\n",
    "indxHB = np.where(stimCategory == 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aea81f",
   "metadata": {},
   "source": [
    "The encoding table also has 3 types of boundary times. For the purposes of our demonstration, we will focus on boundary1 times, and extract them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e13c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get timings of Boundary1\n",
    "boundary1_time = np.array(encoding_table.boundary1_time)\n",
    "\n",
    "# What does this look like?\n",
    "print(boundary1_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4070c05",
   "metadata": {},
   "source": [
    "This contains the timings of all boundaries in this block of trials. Note that we also have the type of boundary for each trial. Let's store the NB and HB boundary timings in separate variables, as Pynapple Ts objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101376c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB timings\n",
    "NB = \n",
    "\n",
    "# HB timings\n",
    "HB = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e664467f",
   "metadata": {},
   "source": [
    "Now the analysis can truly begin! \n",
    "\n",
    "***\n",
    "\n",
    "## Peri-Event Time Histogram (PETH)\n",
    "\n",
    "A PETH is a plot where we align a variable of interest (for example, spikes) to an external event (in this case, to boundary times). This visualization helps us infer relationships between the two.\n",
    "\n",
    "For our demonstration, we will align the spikes of the first unit, which is located in the hippocampus, to the times of NB and HB. You can do a quick check to verify that the first unit is indeed located in the hippocampus, we leave it to you.\n",
    "\n",
    "With Pynapple, PETHs can be computed with a single line of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f907532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PETH of unit aligned to NB, for -0.5 to 1s windows\n",
    "NB_peth = \n",
    "\n",
    "# Compute PETH of unit aligned to HB, for -0.5 to 1s windows\n",
    "HB_peth = \n",
    "\n",
    "#Let's plot the PETH\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1ee614",
   "metadata": {},
   "source": [
    "Awesome! From the PETH, we can see that this neuron fires after boundary onset in HB trials. This is an example of what the authors describe  <a href=\"https://www.nature.com/articles/s41593-022-01020-w\" target=\"_blank\">here</a> as a *boundary cell*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3b9d52",
   "metadata": {},
   "source": [
    "*** \n",
    "\n",
    "### PETH of firing rate for NB and HB trials\n",
    "\n",
    "Now that we have the PETH of spiking, we can go one step further. We will plot the mean firing rate of this cell aligned to the boundary for each trial type. Doing this in Pynapple is very simple!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf4f333",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_size = 0.2 #200ms bin size\n",
    "step_size = 0.01 #10ms step size, to make overlapping bins \n",
    "winsize = int(bin_size/step_size) #Window size \n",
    "\n",
    "#Use Pynapple to compute binned spike counts\n",
    "counts_NB =                                #Spike counts binned in 10ms steps, for NB trials\n",
    "counts_HB =                                #Spike counts binned in 10ms steps, for HB trials\n",
    "\n",
    "#Smoothing binned spike counts using a window of size 20, for both trial types\n",
    "counts_NB = counts_NB.as_dataframe().rolling(winsize, win_type = 'gaussian', min_periods = 1, center = True, axis = 0).mean(std = 0.2 * winsize) \n",
    "counts_HB = counts_HB.as_dataframe().rolling(winsize, win_type = 'gaussian', min_periods = 1, center = True, axis = 0).mean(std = 0.2 * winsize) \n",
    "\n",
    "#Compute firing rate for both trial types\n",
    "fr_NB = counts_NB * winsize\n",
    "fr_HB = counts_HB * winsize\n",
    "\n",
    "#Compute the mean firing rate for both trial types \n",
    "meanfr_NB = \n",
    "meanfr_HB = \n",
    "\n",
    "#Compute standard error of mean (SEM) of the firing rate for both trial types\n",
    "error_NB = \n",
    "error_HB = \n",
    "\n",
    "#Plot the mean +/- SEM of firing rate for both trial types\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa4670b",
   "metadata": {},
   "source": [
    "This plot verifies what we visualized in the PETH rasters above, that this cell responds to a hard boundary. Hence, it is a *boundary cell*. To learn more about these cells, please check out the original study <a href=\"https://www.nature.com/articles/s41593-022-01020-w\" target=\"_blank\">here</a>. \n",
    "\n",
    "I hope this tutorial was helpful. If you have any questions, comments or suggestions, please feel free to reach out to the Pynapple Team!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
